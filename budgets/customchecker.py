
# Start of BODY
'''
TestStruct::
testcase_id                   [int] ID of the test-case
testcase_input_path           [str] File path to test-case input
testcase_output_path          [str] File path to test-case output generated by the problem solver
testcase_expected_output_path [str] File path to test-case expected output to be matched with
testcase_error_path           [str] File path to test-case STDERR
metadata_file_paths           [list<str>] File paths to Question metadata (Extra files usually used for defining traning sets)
submission_code_path          [str] File path to submission source code
submission_language           [str] Language token of submission
testcase_result               [bool] Set to True if test-case output matches test-case expected output. Matching is done line by line
testcase_signal               [int] Exit code of the test-case process
testcase_time                 [float] Time taken by the test-case process in seconds
testcase_memory               [int] Peak memory of the test-case process determined in bytes
data                          [str] <Future use>
ResultStruct::
result      [bool]  Assign test-case result. True determines success. False determines failure
score       [float] Assign test-case score. Normalized between 0 to 1
message     [str] Assign test-case message. This message is visible to the problem solver
'''


def run_custom_checker(t_obj, r_obj):
    # Don't print anything to STDOUT in this function
    # Enter your custom checker scoring logic here
    expectfile = open(t_obj.testcase_expected_output_path, "r")
    outputfile = open(t_obj.testcase_output_path, "r")
    expected = int(expectfile.read())
    output = int(outputfile.read())
    
    r_obj.result = False
    r_obj.score = 0.0
    r_obj.message = "Failure"
    
    if (output > expected*0.9 and output < expected*1.1):
        r_obj.result = True
        r_obj.score = 1.0
        r_obj.message = "Success"


# End of BODY
        
